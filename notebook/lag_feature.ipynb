{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464biteddf527578464ed5a56435cc8d4ac68f",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, metrics\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\"\"\"\n",
    "TODO 365 + 28日分をtrainデータとしてlag特徴量を作成し、古い28日分は欠損値が出るので削除\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# メモリ使用量の削減\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# function to read the data and merge it\n",
    "# (ignoring some columns, this is a very fst model)\n",
    "def read_data():\n",
    "    print('Reading files...')\n",
    "\n",
    "    calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n",
    "    calendar_df = reduce_mem_usage(calendar_df)\n",
    "    print('Calendar: ' + str(calendar_df.shape))\n",
    "\n",
    "    sell_prices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "    sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "    print('Sell prices: ' + str(sell_prices_df.shape))\n",
    "\n",
    "    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "    print('Sales train validation: ' + str(train_df.shape))\n",
    "\n",
    "    submission_df = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "    print(\"Submission: \" + str(submission_df.shape))\n",
    "\n",
    "    return calendar_df, sell_prices_df, train_df, submission_df\n",
    "\n",
    "\n",
    "def melt_and_merge(calendar_df, sell_prices_df, train_df, submission_df, num_train_data):\n",
    "\n",
    "    # trainは直近１年間のデータのみ使用\n",
    "    drop_columns = [f\"d_{d}\" for d in range(1, (1913 - num_train_data) + 1)]\n",
    "    train_df.drop(drop_columns, inplace = True, axis=1)\n",
    "    print(\"\\ntrainは直近１年間のデータのみ使用\")\n",
    "    print('Sales train validation(remain only one year): ' + str(train_df.shape))\n",
    "\n",
    "    # 商品情報を抽出\n",
    "    product_df = train_df.loc[:, \"id\":\"state_id\"]\n",
    "\n",
    "    # 列方向に連なっていたのを変形し行方向に連ねるように整理\n",
    "    train_df = pd.melt(train_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                       var_name='day', value_name='demand')\n",
    "\n",
    "    train_day = train_df[\"day\"].unique()\n",
    "    print(\"train_data: {0} ~ {1} -> {2}\".format(train_day[0], train_day[-1], len(train_day)))\n",
    "\n",
    "    # seperate test dataframes\n",
    "    stage1_eval_df = submission_df[submission_df[\"id\"].str.contains(\"validation\")]\n",
    "    stage2_eval_df = submission_df[submission_df[\"id\"].str.contains(\"evaluation\")]\n",
    "\n",
    "    # change column names\n",
    "    stage1_eval_df.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1942)]  # F1 ~ F28 => d_1914 ~ d_1941\n",
    "    stage2_eval_df.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1970)]  # F1 ~ F28 => d_1942 ~ d_1969\n",
    "\n",
    "    # melt, mergeを使ってsubmission用のdataframeを上のsales_train_validationと同様の形式に変形\n",
    "    stage1_eval_df = stage1_eval_df.merge(product_df, how='left', on='id')\n",
    "    stage1_eval_df = pd.melt(stage1_eval_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                             var_name='day', value_name='demand')\n",
    "    stage1_day = stage1_eval_df[\"day\"].unique()\n",
    "    print(\"[STAGE1] eval_data: {0} ~ {1} -> {2}\".format(stage1_day[0], stage1_day[-1], len(stage1_day)))\n",
    "\n",
    "    # train_df, stage1_eval_dfと同様にstage2_eval_dfとproduct_dfをmergeさせたい\n",
    "    # しかしidが_evaluationのままだとデータが一致せずmergeできないので一時的に_validationにidを変更\n",
    "    stage2_eval_df['id'] = stage2_eval_df.loc[:, 'id'].str.replace('_evaluation', '_validation')\n",
    "    stage2_eval_df = stage2_eval_df.merge(product_df, how='left', on='id')\n",
    "    stage2_eval_df['id'] = stage2_eval_df.loc[:, 'id'].str.replace('_validation', '_evaluation')\n",
    "    stage2_eval_df = pd.melt(stage2_eval_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                             var_name='day', value_name='demand')\n",
    "    stage2_day = stage2_eval_df[\"day\"].unique()\n",
    "    print(\"[STAGE2] eval_data: {0} ~ {1} -> {2}\".format(stage2_day[0], stage2_day[-1], len(stage2_day)))\n",
    "\n",
    "    train_df['part'] = 'train'\n",
    "    stage1_eval_df['part'] = 'stage1'\n",
    "    stage2_eval_df['part'] = 'stage2'\n",
    "\n",
    "    data_df = pd.concat([train_df, stage1_eval_df, stage2_eval_df], axis=0)\n",
    "    data_df = reduce_mem_usage(data_df)\n",
    "    # print(\"\\n[INFO] data_df(after merge valid & eval) ->\")\n",
    "    # data_df.head()\n",
    "\n",
    "    # 不要なdataframeの削除\n",
    "    del train_df, stage1_eval_df, stage2_eval_df, product_df\n",
    "\n",
    "    # drop some calendar features\n",
    "    calendar_df.drop(['weekday', 'wday', 'month', 'year'], inplace=True, axis=1)\n",
    "\n",
    "    # delete stage2_eval_df for now\n",
    "    data_df = data_df[data_df['part'] != 'stage2']\n",
    "    print(\"[CHECK] Remove the stage2 eval data\")\n",
    "\n",
    "    # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "    data_df = pd.merge(data_df, calendar_df, how='left', left_on=['day'], right_on=['d'])\n",
    "    data_df.drop('d', inplace=True, axis=1)\n",
    "\n",
    "    # get the sell price data (this feature should be very important)\n",
    "    data_df = data_df.merge(sell_prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    # print(\"\\n[INFO] data_df(after merge calendar & prices) ->\")\n",
    "    # print(data_df.head(5))\n",
    "    # print(data_df.columns)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# label encoding\n",
    "def encode_categorical(data_df):\n",
    "    nan_features = ['event_name_1', 'event_type_1',\n",
    "                    'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        # label encodingのためnanを文字列に変換\n",
    "        data_df[feature].fillna('unknown', inplace=True)\n",
    "\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "           'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data_df[feature] = encoder.fit_transform(data_df[feature])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Reading files...\nCalendar: (1969, 14)\nSell prices: (6841121, 4)\nSales train validation: (30490, 1919)\nSubmission: (60980, 29)\n\ntrainは直近１年間のデータのみ使用\nSales train validation(remain only one year): (30490, 427)\ntrain_data: d_1493 ~ d_1913 -> 421\n[STAGE1] eval_data: d_1914 ~ d_1941 -> 28\n[STAGE2] eval_data: d_1942 ~ d_1969 -> 28\n[CHECK] Remove the stage2 eval data\n"
    }
   ],
   "source": [
    "calendar_df, sell_prices_df, train_df, submission_df = read_data()\n",
    "\n",
    "# preprocessing\n",
    "num_train_data = 421\n",
    "data_df = melt_and_merge(calendar_df, sell_prices_df, train_df, submission_df, num_train_data)\n",
    "data_df = encode_categorical(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量エンジニアリング\n",
    "def feature_engineering(data_df):\n",
    "    \"\"\"\n",
    "    1日後のリード特徴量\n",
    "    1日前のラグ特徴量\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n[START] feature engineering ->\")\n",
    "\n",
    "    # ラグ特徴量\n",
    "    data_df['lag7'] = data_df.groupby(['id'])['demand'].shift(7)\n",
    "    data_df['lag28'] = data_df.groupby(['id'])['demand'].shift(28)\n",
    "    data_df['rmean_lag7_7'] = data_df.groupby(['id'])['lag7'].transform(lambda x: x.rolling(7).mean())\n",
    "    data_df['rmean_lag7_28'] = data_df.groupby(['id'])['lag7'].transform(lambda x: x.rolling(28).mean())\n",
    "    data_df['rmean_lag28_7'] = data_df.groupby(['id'])['lag28'].transform(lambda x: x.rolling(7).mean())\n",
    "    data_df['rmean_lag28_28'] = data_df.groupby(['id'])['lag28'].transform(lambda x: x.rolling(28).mean())\n",
    "\n",
    "        \n",
    "    # price features\n",
    "    # data_df['sell_price_lag1'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    # data_df['sell_price_lag7'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(7))\n",
    "    # data_df['sell_price_lag28'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(28))\n",
    "    # mean_sell_price_df = data_df.groupby('id').mean()\n",
    "    # mean_sell_price_df.rename(columns={\"sell_price\": \"mean_sell_price\"}, inplace=True)\n",
    "    # data_df = data_df.merge(mean_sell_price_df[\"mean_sell_price\"], on=\"id\")\n",
    "    # data_df[\"diff_sell_price\"] = data_df[\"sell_price\"] - data_df[\"mean_sell_price\"]\n",
    "    # data_df[\"div_sell_price\"] = data_df[\"sell_price\"] / data_df[\"mean_sell_price\"]\n",
    "\n",
    "    # time features\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    data_df['year'] = data_df['date'].dt.year.astype(np.int16)\n",
    "    data_df['quarter'] = data_df['date'].dt.quarter.astype(np.int8)\n",
    "    data_df['month'] = data_df['date'].dt.month.astype(np.int8)\n",
    "    data_df['week'] = data_df['date'].dt.week.astype(np.int8)\n",
    "    data_df['mday'] = data_df['date'].dt.day.astype(np.int8)\n",
    "    data_df['wday'] = data_df['date'].dt.dayofweek.astype(np.int8)\n",
    "    # data_df['is_year_end'] = data_df['date'].dt.is_year_end.astype(np.int8)\n",
    "    # data_df['is_year_start'] = data_df['date'].dt.is_year_start.astype.astype(np.int8)\n",
    "    # data_df['is_quarter_end'] = data_df['date'].dt.is_quarter_end.astype(np.int8)\n",
    "    # data_df['is_quarter_start'] = data_df['date'].is_quarter_start.astype(np.int8)\n",
    "    # data_df['is_month_end'] = data_df['date'].dt.is_month_end.astype(np.int8)\n",
    "    # data_df['is_month_start'] = data_df['date'].dt.is_month_start.astype(np.int8)\n",
    "    # data_df[\"is_weekend\"] = data_df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "\n",
    "    # black friday\n",
    "    black_friday = [\"2011-11-25\", \"2012-11-23\", \"2013-11-29\", \"2014-11-28\", \"2015-11-27\"]\n",
    "    data_df[\"black_friday\"] = data_df[\"date\"].isin(black_friday) * 1\n",
    "\n",
    "    print(\"[FINISH] feature engineering\")\n",
    "\n",
    "    # lag特徴量によって欠損している部分を削除\n",
    "    print(\"lag featureによって欠損している部分を削除\")\n",
    "    print(\"Before train data:{0} ~ {1}\".format(data_df[\"day\"].unique()[0], data_df[\"day\"].unique()[-1]))\n",
    "    # data_df = data_df[data_df[\"day\"] >= \"d_1549\"]\n",
    "    data_df.dropna(inplace = True)\n",
    "    print(\"↓\")\n",
    "    print(\"After train data:{0} ~ {1}\".format(data_df[\"day\"].unique()[0], data_df[\"day\"].unique()[-1]))\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[START] feature engineering ->\n[FINISH] feature engineering\nlag featureによって欠損している部分を削除\nBefore train data:d_1493 ~ d_1941\n↓\nAfter train data:d_1548 ~ d_1941\n"
    }
   ],
   "source": [
    "data_df = feature_engineering(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 11980608 entries, 1676950 to 13690009\nData columns (total 32 columns):\nid                object\nitem_id           int64\ndept_id           int64\ncat_id            int64\nstore_id          int64\nstate_id          int64\nday               object\ndemand            int16\npart              object\ndate              datetime64[ns]\nwm_yr_wk          int16\nevent_name_1      int64\nevent_type_1      int64\nevent_name_2      int64\nevent_type_2      int64\nsnap_CA           int8\nsnap_TX           int8\nsnap_WI           int8\nsell_price        float16\nlag7              float64\nlag28             float64\nrmean_lag7_7      float64\nrmean_lag7_28     float64\nrmean_lag28_7     float64\nrmean_lag28_28    float64\nyear              int16\nquarter           int8\nmonth             int8\nweek              int8\nmday              int8\nwday              int8\nblack_friday      int64\ndtypes: datetime64[ns](1), float16(1), float64(6), int16(3), int64(10), int8(8), object(3)\nmemory usage: 2.4+ GB\n"
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['d_1548', 'd_1549', 'd_1550', 'd_1551', 'd_1552', 'd_1553',\n       'd_1554', 'd_1555', 'd_1556', 'd_1557', 'd_1558', 'd_1559',\n       'd_1560', 'd_1561', 'd_1562', 'd_1563', 'd_1564', 'd_1565',\n       'd_1566', 'd_1567', 'd_1568', 'd_1569', 'd_1570', 'd_1571',\n       'd_1572', 'd_1573', 'd_1574', 'd_1575', 'd_1576', 'd_1577',\n       'd_1578', 'd_1579', 'd_1580', 'd_1581', 'd_1582', 'd_1583',\n       'd_1584', 'd_1585', 'd_1586', 'd_1587', 'd_1588', 'd_1589',\n       'd_1590', 'd_1591', 'd_1592', 'd_1593', 'd_1594', 'd_1595',\n       'd_1596', 'd_1597', 'd_1598', 'd_1599', 'd_1600', 'd_1601',\n       'd_1602', 'd_1603', 'd_1604', 'd_1605', 'd_1606', 'd_1607',\n       'd_1608', 'd_1609', 'd_1610', 'd_1611', 'd_1612', 'd_1613',\n       'd_1614', 'd_1615', 'd_1616', 'd_1617', 'd_1618', 'd_1619',\n       'd_1620', 'd_1621', 'd_1622', 'd_1623', 'd_1624', 'd_1625',\n       'd_1626', 'd_1627', 'd_1628', 'd_1629', 'd_1630', 'd_1631',\n       'd_1632', 'd_1633', 'd_1634', 'd_1635', 'd_1636', 'd_1637',\n       'd_1638', 'd_1639', 'd_1640', 'd_1641', 'd_1642', 'd_1643',\n       'd_1644', 'd_1645', 'd_1646', 'd_1647', 'd_1648', 'd_1649',\n       'd_1650', 'd_1651', 'd_1652', 'd_1653', 'd_1654', 'd_1655',\n       'd_1656', 'd_1657', 'd_1658', 'd_1659', 'd_1660', 'd_1661',\n       'd_1662', 'd_1663', 'd_1664', 'd_1665', 'd_1666', 'd_1667',\n       'd_1668', 'd_1669', 'd_1670', 'd_1671', 'd_1672', 'd_1673',\n       'd_1674', 'd_1675', 'd_1676', 'd_1677', 'd_1678', 'd_1679',\n       'd_1680', 'd_1681', 'd_1682', 'd_1683', 'd_1684', 'd_1685',\n       'd_1686', 'd_1687', 'd_1688', 'd_1689', 'd_1690', 'd_1691',\n       'd_1692', 'd_1693', 'd_1694', 'd_1695', 'd_1696', 'd_1697',\n       'd_1698', 'd_1699', 'd_1700', 'd_1701', 'd_1702', 'd_1703',\n       'd_1704', 'd_1705', 'd_1706', 'd_1707', 'd_1708', 'd_1709',\n       'd_1710', 'd_1711', 'd_1712', 'd_1713', 'd_1714', 'd_1715',\n       'd_1716', 'd_1717', 'd_1718', 'd_1719', 'd_1720', 'd_1721',\n       'd_1722', 'd_1723', 'd_1724', 'd_1725', 'd_1726', 'd_1727',\n       'd_1728', 'd_1729', 'd_1730', 'd_1731', 'd_1732', 'd_1733',\n       'd_1734', 'd_1735', 'd_1736', 'd_1737', 'd_1738', 'd_1739',\n       'd_1740', 'd_1741', 'd_1742', 'd_1743', 'd_1744', 'd_1745',\n       'd_1746', 'd_1747', 'd_1748', 'd_1749', 'd_1750', 'd_1751',\n       'd_1752', 'd_1753', 'd_1754', 'd_1755', 'd_1756', 'd_1757',\n       'd_1758', 'd_1759', 'd_1760', 'd_1761', 'd_1762', 'd_1763',\n       'd_1764', 'd_1765', 'd_1766', 'd_1767', 'd_1768', 'd_1769',\n       'd_1770', 'd_1771', 'd_1772', 'd_1773', 'd_1774', 'd_1775',\n       'd_1776', 'd_1777', 'd_1778', 'd_1779', 'd_1780', 'd_1781',\n       'd_1782', 'd_1783', 'd_1784', 'd_1785', 'd_1786', 'd_1787',\n       'd_1788', 'd_1789', 'd_1790', 'd_1791', 'd_1792', 'd_1793',\n       'd_1794', 'd_1795', 'd_1796', 'd_1797', 'd_1798', 'd_1799',\n       'd_1800', 'd_1801', 'd_1802', 'd_1803', 'd_1804', 'd_1805',\n       'd_1806', 'd_1807', 'd_1808', 'd_1809', 'd_1810', 'd_1811',\n       'd_1812', 'd_1813', 'd_1814', 'd_1815', 'd_1816', 'd_1817',\n       'd_1818', 'd_1819', 'd_1820', 'd_1821', 'd_1822', 'd_1823',\n       'd_1824', 'd_1825', 'd_1826', 'd_1827', 'd_1828', 'd_1829',\n       'd_1830', 'd_1831', 'd_1832', 'd_1833', 'd_1834', 'd_1835',\n       'd_1836', 'd_1837', 'd_1838', 'd_1839', 'd_1840', 'd_1841',\n       'd_1842', 'd_1843', 'd_1844', 'd_1845', 'd_1846', 'd_1847',\n       'd_1848', 'd_1849', 'd_1850', 'd_1851', 'd_1852', 'd_1853',\n       'd_1854', 'd_1855', 'd_1856', 'd_1857', 'd_1858', 'd_1859',\n       'd_1860', 'd_1861', 'd_1862', 'd_1863', 'd_1864', 'd_1865',\n       'd_1866', 'd_1867', 'd_1868', 'd_1869', 'd_1870', 'd_1871',\n       'd_1872', 'd_1873', 'd_1874', 'd_1875', 'd_1876', 'd_1877',\n       'd_1878', 'd_1879', 'd_1880', 'd_1881', 'd_1882', 'd_1883',\n       'd_1884', 'd_1885', 'd_1886', 'd_1887', 'd_1888', 'd_1889',\n       'd_1890', 'd_1891', 'd_1892', 'd_1893', 'd_1894', 'd_1895',\n       'd_1896', 'd_1897', 'd_1898', 'd_1899', 'd_1900', 'd_1901',\n       'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906', 'd_1907',\n       'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913',\n       'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n       'd_1938', 'd_1939', 'd_1940', 'd_1941'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "data_df[\"day\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3049"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data_df[data_df[\"day\"]==\"d_1913\"][\"item_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1913 = data_df[data_df[\"day\"]==\"d_1913\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12805800    1.0\nName: lag28, dtype: float64"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "d_1913[d_1913[\"id\"] == \"HOBBIES_1_001_CA_1_validation\"][\"lag28\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12805800   2016-04-24\n12805801   2016-04-24\n12805802   2016-04-24\n12805803   2016-04-24\n12805804   2016-04-24\n12805805   2016-04-24\n12805806   2016-04-24\n12805807   2016-04-24\n12805808   2016-04-24\n12805809   2016-04-24\n12805810   2016-04-24\n12805811   2016-04-24\n12805812   2016-04-24\n12805813   2016-04-24\n12805814   2016-04-24\n12805815   2016-04-24\n12805816   2016-04-24\n12805817   2016-04-24\n12805818   2016-04-24\n12805819   2016-04-24\n12805820   2016-04-24\n12805821   2016-04-24\n12805822   2016-04-24\n12805823   2016-04-24\n12805824   2016-04-24\n12805825   2016-04-24\n12805826   2016-04-24\n12805827   2016-04-24\n12805828   2016-04-24\n12805829   2016-04-24\n              ...    \n12836260   2016-04-24\n12836261   2016-04-24\n12836262   2016-04-24\n12836263   2016-04-24\n12836264   2016-04-24\n12836265   2016-04-24\n12836266   2016-04-24\n12836267   2016-04-24\n12836268   2016-04-24\n12836269   2016-04-24\n12836270   2016-04-24\n12836271   2016-04-24\n12836272   2016-04-24\n12836273   2016-04-24\n12836274   2016-04-24\n12836275   2016-04-24\n12836276   2016-04-24\n12836277   2016-04-24\n12836278   2016-04-24\n12836279   2016-04-24\n12836280   2016-04-24\n12836281   2016-04-24\n12836282   2016-04-24\n12836283   2016-04-24\n12836284   2016-04-24\n12836285   2016-04-24\n12836286   2016-04-24\n12836287   2016-04-24\n12836288   2016-04-24\n12836289   2016-04-24\nName: date, Length: 30490, dtype: datetime64[ns]"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data_df[data_df[\"day\"]==\"d_1913\"][\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}