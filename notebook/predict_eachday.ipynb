{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464biteddf527578464ed5a56435cc8d4ac68f",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータを1日ずつ予測するための検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, metrics\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\"\"\"\n",
    "TODO 365 + 28日分をtrainデータとしてlag特徴量を作成し、古い28日分は欠損値が出るので削除\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# メモリ使用量の削減\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# function to read the data and merge it\n",
    "# (ignoring some columns, this is a very fst model)\n",
    "def read_data():\n",
    "    print('Reading files...')\n",
    "\n",
    "    calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n",
    "    calendar_df = reduce_mem_usage(calendar_df)\n",
    "    print('Calendar: ' + str(calendar_df.shape))\n",
    "\n",
    "    sell_prices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "    sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "    print('Sell prices: ' + str(sell_prices_df.shape))\n",
    "\n",
    "    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "    print('Sales train validation: ' + str(train_df.shape))\n",
    "\n",
    "    submission_df = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "    print(\"Submission: \" + str(submission_df.shape))\n",
    "\n",
    "    return calendar_df, sell_prices_df, train_df, submission_df\n",
    "\n",
    "\n",
    "def melt_and_merge(calendar_df, sell_prices_df, train_df, submission_df, num_train_data):\n",
    "\n",
    "    # trainは直近１年間のデータのみ使用\n",
    "    drop_columns = [f\"d_{d}\" for d in range(1, (1913 - num_train_data) + 1)]\n",
    "    train_df.drop(drop_columns, inplace = True, axis=1)\n",
    "    print(\"\\ntrainは直近１年間のデータのみ使用\")\n",
    "    print('Sales train validation(remain only one year): ' + str(train_df.shape))\n",
    "\n",
    "    # 商品情報を抽出\n",
    "    product_df = train_df.loc[:, \"id\":\"state_id\"]\n",
    "\n",
    "    # 列方向に連なっていたのを変形し行方向に連ねるように整理\n",
    "    train_df = pd.melt(train_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                       var_name='day', value_name='demand')\n",
    "\n",
    "    train_day = train_df[\"day\"].unique()\n",
    "    print(\"train_data: {0} ~ {1} -> {2}\".format(train_day[0], train_day[-1], len(train_day)))\n",
    "\n",
    "    # seperate test dataframes\n",
    "    stage1_eval_df = submission_df[submission_df[\"id\"].str.contains(\"validation\")]\n",
    "    stage2_eval_df = submission_df[submission_df[\"id\"].str.contains(\"evaluation\")]\n",
    "\n",
    "    # change column names\n",
    "    stage1_eval_df.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1942)]  # F1 ~ F28 => d_1914 ~ d_1941\n",
    "    stage2_eval_df.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1970)]  # F1 ~ F28 => d_1942 ~ d_1969\n",
    "\n",
    "    # melt, mergeを使ってsubmission用のdataframeを上のsales_train_validationと同様の形式に変形\n",
    "    stage1_eval_df = stage1_eval_df.merge(product_df, how='left', on='id')\n",
    "    stage1_eval_df = pd.melt(stage1_eval_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                             var_name='day', value_name='demand')\n",
    "    stage1_day = stage1_eval_df[\"day\"].unique()\n",
    "    print(\"[STAGE1] eval_data: {0} ~ {1} -> {2}\".format(stage1_day[0], stage1_day[-1], len(stage1_day)))\n",
    "\n",
    "    # train_df, stage1_eval_dfと同様にstage2_eval_dfとproduct_dfをmergeさせたい\n",
    "    # しかしidが_evaluationのままだとデータが一致せずmergeできないので一時的に_validationにidを変更\n",
    "    stage2_eval_df['id'] = stage2_eval_df.loc[:, 'id'].str.replace('_evaluation', '_validation')\n",
    "    stage2_eval_df = stage2_eval_df.merge(product_df, how='left', on='id')\n",
    "    stage2_eval_df['id'] = stage2_eval_df.loc[:, 'id'].str.replace('_validation', '_evaluation')\n",
    "    stage2_eval_df = pd.melt(stage2_eval_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                             var_name='day', value_name='demand')\n",
    "    stage2_day = stage2_eval_df[\"day\"].unique()\n",
    "    print(\"[STAGE2] eval_data: {0} ~ {1} -> {2}\".format(stage2_day[0], stage2_day[-1], len(stage2_day)))\n",
    "\n",
    "    train_df['part'] = 'train'\n",
    "    stage1_eval_df['part'] = 'stage1'\n",
    "    stage2_eval_df['part'] = 'stage2'\n",
    "\n",
    "    data_df = pd.concat([train_df, stage1_eval_df, stage2_eval_df], axis=0)\n",
    "    data_df = reduce_mem_usage(data_df)\n",
    "    # print(\"\\n[INFO] data_df(after merge valid & eval) ->\")\n",
    "    # data_df.head()\n",
    "\n",
    "    # 不要なdataframeの削除\n",
    "    del train_df, stage1_eval_df, stage2_eval_df, product_df\n",
    "\n",
    "    # drop some calendar features\n",
    "    calendar_df.drop(['weekday', 'wday', 'month', 'year'], inplace=True, axis=1)\n",
    "\n",
    "    # delete stage2_eval_df for now\n",
    "    data_df = data_df[data_df['part'] != 'stage2']\n",
    "    print(\"[CHECK] Remove the stage2 eval data\")\n",
    "\n",
    "    # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "    data_df = pd.merge(data_df, calendar_df, how='left', left_on=['day'], right_on=['d'])\n",
    "    data_df.drop('d', inplace=True, axis=1)\n",
    "\n",
    "    # get the sell price data (this feature should be very important)\n",
    "    data_df = data_df.merge(sell_prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    # print(\"\\n[INFO] data_df(after merge calendar & prices) ->\")\n",
    "    # print(data_df.head(5))\n",
    "    # print(data_df.columns)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# label encoding\n",
    "def encode_categorical(data_df):\n",
    "    nan_features = ['event_name_1', 'event_type_1',\n",
    "                    'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        # label encodingのためnanを文字列に変換\n",
    "        data_df[feature].fillna('unknown', inplace=True)\n",
    "\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "           'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data_df[feature] = encoder.fit_transform(data_df[feature])\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def data_split(data_df):\n",
    "    # train data\n",
    "    train_df = data_df[data_df['part'] == 'train']\n",
    "\n",
    "    # stage1 eval data(特徴量生成用に56日分多めに確保しておく)\n",
    "    test_df = data_df[(data_df['day'] > 'd_1857')]  # 56日前~(lag特徴量生成に使用) -> 1914(予測対象) ~\n",
    "\n",
    "    del data_df\n",
    "    gc.collect()\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def feature_engineering(data_df):\n",
    "    \"\"\"\n",
    "    1日後のリード特徴量\n",
    "    1日前のラグ特徴量\n",
    "    \"\"\"\n",
    "\n",
    "    # ラグ特徴量\n",
    "    data_df['lag7'] = data_df.groupby(['id'])['demand'].shift(7)\n",
    "    data_df['lag28'] = data_df.groupby(['id'])['demand'].shift(28)\n",
    "    data_df['rmean_lag7_7'] = data_df.groupby(['id'])['lag7'].transform(lambda x: x.rolling(7).mean())\n",
    "    data_df['rmean_lag7_28'] = data_df.groupby(['id'])['lag7'].transform(lambda x: x.rolling(28).mean())\n",
    "    data_df['rmean_lag28_7'] = data_df.groupby(['id'])['lag28'].transform(lambda x: x.rolling(7).mean())\n",
    "    data_df['rmean_lag28_28'] = data_df.groupby(['id'])['lag28'].transform(lambda x: x.rolling(28).mean())\n",
    "\n",
    "    # price features\n",
    "    # data_df['sell_price_lag1'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    # data_df['sell_price_lag7'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(7))\n",
    "    # data_df['sell_price_lag28'] = data_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(28))\n",
    "    # mean_sell_price_df = data_df.groupby('id').mean()\n",
    "    # mean_sell_price_df.rename(columns={\"sell_price\": \"mean_sell_price\"}, inplace=True)\n",
    "    # data_df = data_df.merge(mean_sell_price_df[\"mean_sell_price\"], on=\"id\")\n",
    "    # data_df[\"diff_sell_price\"] = data_df[\"sell_price\"] - data_df[\"mean_sell_price\"]\n",
    "    # data_df[\"div_sell_price\"] = data_df[\"sell_price\"] / data_df[\"mean_sell_price\"]\n",
    "\n",
    "    # time features\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    # data_df['year'] = data_df['date'].dt.year.astype(np.int16)\n",
    "    # data_df['quarter'] = data_df['date'].dt.quarter.astype(np.int8)\n",
    "    # data_df['month'] = data_df['date'].dt.month.astype(np.int8)\n",
    "    # data_df['week'] = data_df['date'].dt.week.astype(np.int8)\n",
    "    # data_df['mday'] = data_df['date'].dt.day.astype(np.int8)\n",
    "    # data_df['wday'] = data_df['date'].dt.dayofweek.astype(np.int8)\n",
    "    # data_df['is_year_end'] = data_df['date'].dt.is_year_end.astype(np.int8)\n",
    "    # data_df['is_year_start'] = data_df['date'].dt.is_year_start.astype.astype(np.int8)\n",
    "    # data_df['is_quarter_end'] = data_df['date'].dt.is_quarter_end.astype(np.int8)\n",
    "    # data_df['is_quarter_start'] = data_df['date'].is_quarter_start.astype(np.int8)\n",
    "    # data_df['is_month_end'] = data_df['date'].dt.is_month_end.astype(np.int8)\n",
    "    # data_df['is_month_start'] = data_df['date'].dt.is_month_start.astype(np.int8)\n",
    "    # data_df[\"is_weekend\"] = data_df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "\n",
    "    # black friday\n",
    "    # black_friday = [\"2011-11-25\", \"2012-11-23\", \"2013-11-29\", \"2014-11-28\", \"2015-11-27\"]\n",
    "    # data_df[\"black_friday\"] = data_df[\"date\"].isin(black_friday) * 1\n",
    "    # data_df[\"black_friday\"] = data_df[\"black_friday\"].astype(np.int8)\n",
    "\n",
    "    # lag特徴量によって欠損している部分を削除\n",
    "    data_df.dropna(inplace = True)\n",
    "    data_df = reduce_mem_usage(data_df)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# 交差検証はランダムサンプリングで疑似的に行う\n",
    "def train_val_split(train_df):\n",
    "    # train data\n",
    "    X_train = train_df[train_df['part'] <= 'train']\n",
    "    y_train = X_train['demand']\n",
    "\n",
    "    # valid data\n",
    "    # This is just a subsample of the training set, not a real validation set !\n",
    "    print(\"\\n[CHECK] This valid data is not a real valid data, just random sampling data from train data!\")\n",
    "    fake_valid_inds = np.random.choice(len(X_train), 1000000)\n",
    "    print(fake_valid_inds)\n",
    "    X_val = X_train.iloc[fake_valid_inds]\n",
    "    y_val = X_val[\"demand\"]\n",
    "\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "# # 交差検証は時系列に沿ったhold-out法で行う\n",
    "# def data_split(data_df):\n",
    "#     # train data\n",
    "#     X_train = data_df[data_df['day'] <= 'd_1885']\n",
    "#     y_train = X_train['demand']\n",
    "\n",
    "#     # valid data\n",
    "#     X_val = data_df[(data_df['day'] > 'd_1885') & (data_df['day'] <= 'd_1913')]\n",
    "#     y_val = X_val['demand']\n",
    "\n",
    "#     # stage1 eval data\n",
    "#     test = data_df[(data_df['day'] > 'd_1913')]\n",
    "\n",
    "#     del data_df\n",
    "#     gc.collect()\n",
    "\n",
    "#     return X_train, y_train, X_val, y_val, test\n",
    "\n",
    "\n",
    "def train_lgb(X_train, y_train, X_val, y_val, features, date):\n",
    "    # define random hyperparammeters\n",
    "    params = {'boosting_type': 'gbdt',\n",
    "              'metric': 'rmse',\n",
    "              'objective': \"poisson\",\n",
    "              'n_jobs': -1,\n",
    "              'seed': 5046,\n",
    "              'learning_rate': 0.075,\n",
    "              'lambda_l2': 0.1,\n",
    "              'sub_feature': 0.8,\n",
    "              'sub_row': 0.75,\n",
    "              'bagging_freq': 1}\n",
    "\n",
    "    # datasetの作成\n",
    "    train_set = lgb.Dataset(X_train[features], y_train)\n",
    "    val_set = lgb.Dataset(X_val[features], y_val)\n",
    "\n",
    "    # train/validation\n",
    "    print(\"\\n[START] training model ->\")\n",
    "    model = lgb.train(params, train_set, num_boost_round=2000,  # early_stopping_rounds=200,\n",
    "                      valid_sets=[train_set, val_set], verbose_eval=100)\n",
    "\n",
    "    # save model\n",
    "    model_dir = \"../model/{}\".format(date[:8])\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_name = \"model_{}.pickle\".format(date[9:])\n",
    "    model_path = model_dir + model_name\n",
    "    with open(model_path, mode='wb') as fp:\n",
    "        pickle.dump(model, fp)\n",
    "\n",
    "    val_pred = model.predict(X_val[features], num_iteration=model.best_iteration)\n",
    "    val_RMSE_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n",
    "    print(f'val RMSE score: {val_RMSE_score}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 1日ずつ予測していく\n",
    "def predict_lgb(model, test_df, features):\n",
    "    train_day = test_df[test_df[\"part\"]==\"train\"][\"day\"].unique()  # 特徴量作成のための部分\n",
    "    stage1_day = test_df[test_df[\"part\"]==\"stage1\"][\"day\"].unique()  # 実際に予測する部分\n",
    "    print(\"[START] predict ->\")\n",
    "\n",
    "    for i in tqdm(range(28)):\n",
    "        target_day = np.append(train_day, stage1_day[:i+1])  # 対象の指定(1日ずつ対象範囲を広げていく)\n",
    "        partial_test = test_df[test_df[\"day\"].isin(target_day)].copy()  # 該当するデータを抽出\n",
    "        partial_test = feature_engineering(partial_test)\n",
    "        partial_test = partial_test[partial_test[\"day\"] == stage1_day[i]]  # 予測対象のみ抜粋(1日)\n",
    "        partial_pred = model.predict(partial_test[features])\n",
    "        print(partial_pred)\n",
    "        print(np.unique(partial_pred))\n",
    "        test_df[test_df[\"day\"]==stage1_day[i]]['demand'] = 1.02 * partial_pred\n",
    "        break\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "def result_submission(test_df, submission_df, date):\n",
    "    output_dir = \"../output/{}\".format(date[:8])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_name = \"submission_{}.csv\".format(date[9:])\n",
    "    submission_path = output_dir + submission_name\n",
    "    \n",
    "\n",
    "    pred = test_df[['id', 'date', 'demand']]\n",
    "    pred = pd.pivot(pred, index='id', columns='date', values='demand').reset_index()\n",
    "    pred.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    eval_rows = [row for row in submission_df['id'] if 'evaluation' in row]\n",
    "    eval = submission_df[submission_df['id'].isin(eval_rows)]\n",
    "\n",
    "    valid = submission_df[['id']].merge(pred, on='id')\n",
    "    final = pd.concat([valid, eval])\n",
    "    final.to_csv(submission_path, index=False)\n",
    "    print(\"\\ncompleted process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Reading files...\nCalendar: (1969, 14)\nSell prices: (6841121, 4)\nSales train validation: (30490, 1919)\nSubmission: (60980, 29)\n\ntrainは直近１年間のデータのみ使用\nSales train validation(remain only one year): (30490, 506)\ntrain_data: d_1414 ~ d_1913 -> 500\n[STAGE1] eval_data: d_1914 ~ d_1941 -> 28\n[STAGE2] eval_data: d_1942 ~ d_1969 -> 28\n[CHECK] Remove the stage2 eval data\nN_features: 13\n\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 変更パラメータ\n",
    "num_train_data = 500  # 421 = 365 + 28*2\n",
    "\n",
    "# read data\n",
    "t1 = time.time()\n",
    "date = datetime.datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "calendar_df, sell_prices_df, train_df, submission_df = read_data()\n",
    "\n",
    "# preprocessing\n",
    "data_df = melt_and_merge(calendar_df, sell_prices_df, train_df, submission_df, num_train_data)\n",
    "data_df = encode_categorical(data_df)\n",
    "\n",
    "# data split\n",
    "train_df, test_df = data_split(data_df)\n",
    "\n",
    "# define list of features\n",
    "default_features = ['item_id', 'dept_id', 'store_id','cat_id', 'state_id', 'event_name_1', 'sell_price']\n",
    "demand_features = ['lag7', 'lag28', 'rmean_lag7_7', 'rmean_lag7_28', 'rmean_lag28_7', 'rmean_lag28_28']\n",
    "# price_features = ['sell_price_lag1', 'sell_price_lag7', 'sell_price_lag28', \"diff_sell_price\", \"div_sell_price\"]\n",
    "# time_features = [\"year\", \"month\", \"week\", \"quarter\", \"mday\", \"wday\", \"black_friday\"]\n",
    "features = default_features + demand_features  # + price_features + time_features\n",
    "print(\"N_features: {}\\n\".format(len(features)))\n",
    "\n",
    "# train\n",
    "with open(\"../model/20200408model_115157.pickle\", mode='rb') as fp:\n",
    "    model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day = test_df[test_df[\"part\"]==\"train\"][\"day\"].unique()  # 特徴量作成のための部分\n",
    "stage1_day = test_df[test_df[\"part\"]==\"stage1\"][\"day\"].unique()  # 実際に予測する部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.95573836 0.24375241 0.68493054 ... 1.08343165 1.00747958 1.27688968]\n30490\n"
    }
   ],
   "source": [
    "# predict\n",
    "test_df2 = test_df.copy()\n",
    "for i in range(28):\n",
    "    target_day = np.append(train_day, stage1_day[:i+1])  # 対象の指定(1日ずつ対象範囲を広げていく)\n",
    "    partial_test = test_df2[test_df2[\"day\"].isin(target_day)].copy()  # 該当するデータを抽出\n",
    "    partial_test = feature_engineering(partial_test)\n",
    "    partial_test = partial_test[parge1_day[i]test[\"day\"] == stage1_day[i]]  # 予測対象のみ抜粋(1日)\n",
    "    partial_pred = model.predict(partial_test[features])\n",
    "    print(partial_pred)\n",
    "    print(len(partial_pred))\n",
    "    test_df2[test_df2[\"day\"]==stage1_day[i]]['demand'] = 1.02 * partial_pred\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "30490"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "test_df2[test_df2[\"day\"]==stage1_day[i]]['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.loc[test_df2[\"day\"]==stage1_day[0], \"demand\"] = partial_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "15245000     0.955738\n15245001     0.243752\n15245002     0.684931\n15245003     1.940756\n15245004     1.039538\n15245005     0.990465\n15245006     0.409134\n15245007     7.158406\n15245008     0.761677\n15245009     0.541631\n15245010     0.143886\n15245011     0.201172\n15245012     0.499007\n15245013     1.748715\n15245014     3.355362\n15245015     6.314783\n15245016     1.101404\n15245017     0.049151\n15245018     8.055341\n15245019     0.262996\n15245020     0.639433\n15245021     0.449032\n15245022     1.264680\n15245023     0.121635\n15245024     0.533376\n15245025     0.253089\n15245026     0.326054\n15245027     0.706595\n15245028     1.539399\n15245029     4.262160\n              ...    \n15275460     0.426455\n15275461     0.321897\n15275462     7.271266\n15275463     1.991804\n15275464     0.508307\n15275465     0.626882\n15275466     6.592206\n15275467     1.970911\n15275468     0.768598\n15275469     5.002996\n15275470     0.533904\n15275471     2.347900\n15275472     0.723049\n15275473    13.819510\n15275474     1.323874\n15275475     0.231418\n15275476     2.193332\n15275477     0.246249\n15275478     4.695368\n15275479     0.311365\n15275480     1.780972\n15275481     2.419775\n15275482     1.814426\n15275483     0.736556\n15275484     2.619374\n15275485     0.266380\n15275486     0.322991\n15275487     1.083432\n15275488     1.007480\n15275489     1.276890\nName: demand, Length: 30490, dtype: float64"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test_df2[test_df2[\"day\"]==stage1_day[i]]['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}